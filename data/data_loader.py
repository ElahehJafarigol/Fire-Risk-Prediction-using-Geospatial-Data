# -*- coding: utf-8 -*-
"""data_loader.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hOPyQNMDHknqEYoQEGGJ3JCY8EmHQi29
"""

import os
import h5py
import numpy as np
import pandas as pd
import xarray as xr
from tqdm import tqdm
import geopandas as gpd
from shapely import box
from pyhdf.SD import SD, SDC
from datetime import datetime
from scipy.spatial import cKDTree
from shapely.geometry import Polygon, MultiPolygon

def load_climate_data(humidity_path, temperature_path, wind_path):
    humidity = xr.open_dataset(humidity_path)
    temperature = xr.open_dataset(temperature_path)
    wind = xr.open_dataset(wind_path)
    climate = xr.merge([humidity, temperature, wind])
    climate_df = climate.to_dataframe().reset_index()
    return climate_df

def extract_ndvi(hdf_files, lat_range, lon_range, output_file):
    lat_min, lat_max = lat_range
    lon_min, lon_max = lon_range

    sample_hdf = SD(hdf_files[0], SDC.READ)
    ndvi_sample = sample_hdf.select("250m 16 days NDVI")[:]
    nrows, ncols = ndvi_sample.shape

    lat_values = np.linspace(lat_max, lat_min, nrows)
    lon_values = np.linspace(lon_min, lon_max, ncols)
    lat_grid, lon_grid = np.meshgrid(lat_values, lon_values, indexing="ij")

    lat_flat = lat_grid.ravel()
    lon_flat = lon_grid.ravel()

    for file in tqdm(hdf_files, desc="Processing NDVI Files", unit="file"):
        hdf_file = SD(file, SDC.READ)
        ndvi_data = hdf_file.select("250m 16 days NDVI")[:]
        ndvi_data = np.where(ndvi_data == -3000, np.nan, ndvi_data)

        date_str = file.split(".")[1][1:]
        date_obj = datetime.strptime(date_str, "%Y%j").date()

        ndvi_flat = ndvi_data.ravel()
        ndvi_df = pd.DataFrame({"lat": lat_flat, "lon": lon_flat, "NDVI": ndvi_flat, "date": date_obj})
        ndvi_df.to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)

def load_gedi_data(file_path, beam_name="BEAM0001"):
    with h5py.File(file_path, "r") as hdf_file:
        beam = hdf_file[beam_name]

        elevation = beam["digital_elevation_model"][:]
        elevation = np.where(elevation < 0, np.nan, elevation).flatten()

        e_lat = beam["lat_lowestmode"][:].flatten()
        e_lon = beam["lon_lowestmode"][:].flatten()

        canopy_height = beam['elev_highestreturn'][:]
        canopy_height = np.where(canopy_height < 0, np.nan, canopy_height)

        c_lat = beam["lat_highestreturn"][:].flatten()
        c_lon = beam["lon_highestreturn"][:].flatten()

    elevation_df = pd.DataFrame({"lat": e_lat, "lon": e_lon, "elevation": elevation})
    canopy_df = pd.DataFrame({"lat": c_lat, "lon": c_lon, "canopy_height": canopy_height})

    return elevation_df, canopy_df


def combine_gedi_datasets(gedi_files):
    elevation_frames, canopy_frames = [], []

    for file in gedi_files:
        elevation_df, canopy_df = load_gedi_data(file)
        elevation_frames.append(elevation_df)
        canopy_frames.append(canopy_df)

    combined_elevation_df = pd.concat(elevation_frames, ignore_index=True)
    combined_canopy_df = pd.concat(canopy_frames, ignore_index=True)

    return elevation_df, canopy_df


def spatial_merge_elevation_canopy(elevation_df, canopy_df):
    elevation_gdf = gpd.GeoDataFrame(elevation_df,
                                     geometry=gpd.points_from_xy(elevation_df.lon, elevation_df.lat),
                                     crs="EPSG:4326")

    canopy_gdf = gpd.GeoDataFrame(canopy_df,
                                  geometry=gpd.points_from_xy(canopy_df.lon, canopy_df.lat),
                                  crs="EPSG:4326")

    elevation_gdf = elevation_gdf.to_crs(epsg=3310)
    canopy_gdf = canopy_gdf.to_crs(epsg=3310)

    elevation_canopy_gdf = gpd.sjoin_nearest(elevation_gdf, canopy_gdf,
                                             how="left", distance_col="distance_to_nearest")

    elevation_canopy_gdf = elevation_canopy_gdf.to_crs(epsg=4326)

    elevation_canopy_gdf['lat'] = elevation_canopy_gdf['lat_right']
    elevation_canopy_gdf['lon'] = elevation_canopy_gdf['lon_right']
    elevation_canopy_gdf = elevation_canopy_gdf.drop(['lat_left', 'lon_left', 'lat_right', 'lon_right'], axis=1)
    return elevation_canopy_gdf

def convert_multipolygon(geometry):
    if geometry.geom_type == "MultiPolygon":
        return max(geometry.geoms, key=lambda p: p.area)
    return geometry

def load_fire_data(filepath, bbox):
    fires = gpd.read_file(filepath)
    fires = fires.to_crs(epsg=4326)
    fires = fires[fires.geometry.intersects(bbox)]
    fires["geometry"] = fires["geometry"].apply(convert_multipolygon)
    return fires

def load_infrastructure_data(roads_path, urban_area_path, bbox):
    roads = gpd.read_file(roads_path).to_crs(epsg=4326)
    urban_area = gpd.read_file(urban_area_path).to_crs(epsg=4326)

    roads = roads[roads.geometry.intersects(bbox)]
    urban_area = urban_area[urban_area.geometry.intersects(bbox)]

    return roads, urban_area

def process_fire_data(fire_data, urban_area_data, roads_data):
    projected_crs = "EPSG:3310"  # California Albers (suitable for distance calculations)
    fires = fire_data.to_crs(projected_crs)
    urban_area = urban_area_data.to_crs(projected_crs)
    roads = roads_data.to_crs(projected_crs)

    fires["centroid"] = fires.geometry.centroid

    fires["DISTANCE_TO_URBAN_AREAS"] = fires["centroid"].apply(lambda x: urban_area.distance(x).min())
    fires["DISTANCE_TO_ROADS"] = fires["centroid"].apply(lambda x: roads.distance(x).min())

    fires = fires.to_crs(epsg=4326)

    fires["centroid"] = fires.geometry.centroid
    fires["lat"] = fires.centroid.y
    fires["lon"] = fires.centroid.x

    fires_df = pd.DataFrame(fires)
    fires_df["date"] = pd.to_datetime(fires_df["ALARM_DATE"]).dt.date
    return fires_df

def nearest_neighbor(source_df, target_df, source_cols, target_cols):
    source_coords = source_df[source_cols].to_numpy()
    target_coords = target_df[source_cols].to_numpy()

    tree = cKDTree(target_coords)  # Build KDTree for fast spatial lookup
    distances, indices = tree.query(source_coords, k=1)  # Find nearest match

    return target_df.iloc[indices][target_cols].reset_index(drop=True)

def merge_datasets(fires_df, climate_df, ndvi_df, elevation_canopy_df):
    source_cols = ["lat", "lon"]

    # Merge Climate Data (Temperature, Humidity, Wind Speed)
    fires_df[["T2M", "QV2M", "WS10M"]] = nearest_neighbor(fires_df, climate_df, source_cols, ["T2M", "QV2M", "WS10M"])

    # Merge NDVI Data
    fires_df["NDVI"] = nearest_neighbor(fires_df, ndvi_df, source_cols, ["NDVI"])

    # Merge Elevation & Canopy Height Data
    fires_df["elevation"] = nearest_neighbor(fires_df, elevation_canopy_df, source_cols, ["elevation"])
    fires_df["canopy_height"] = nearest_neighbor(fires_df, elevation_canopy_df, source_cols, ["canopy_height"])

    return fires_df